---
title: "lab6"
author: "Michelle Hernandez"
format:
  html:
    embed-resources: true
editor: visual
---

```{r}

library(data.table)
library(ggplot2)
library(tidytext)
library(readr)
library(dplyr)
```

```{r}
mt_samples <- read_csv("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv")
mt_samples <- mt_samples %>%
  select(description, medical_specialty, transcription)

head(mt_samples)
```

### **Question 1: What specialties do we have?**

```{r}
mt_samples %>%
  count('medical_specialty', sort = TRUE)
# the categories found through this method are overlapping 

unique(mt_samples$medical_specialty)
length(unique(mt_samples$medical_specialty))
# 40 different
```

```{r}
mt_samples
```

### **Question 2**

-   Tokenize the the words in the `transcription` column

-   Count the number of times each token appears

-   Visualize the top 20 most frequent words

    Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r}
mt_samples %>%
  unnest_tokens(words, transcription) %>%
  count(words) %>%
  top_n(20,n)


#mt_samples %>%
 # count('words', sort = TRUE)



#visualize
mt_samples %>%
  unnest_tokens(words, transcription) %>%
  count(words) %>%
  top_n(20,n) %>%
  ggplot(aes(n,words)) +
  geom_col()
```

### **Question 3**

-   Redo visualization but remove stopwords before

-   Bonus points if you remove numbers as well

What do we see know that we have removed stop words? Does it give us a better idea of what the text is about?

```{r}

mt_samples %>%
  unnest_tokens(words, transcription) %>%
  anti_join(stop_words, by=c("words" = "word")) %>%
  count(words, sort=TRUE) %>%
  top_n(20,n) %>%
  ggplot(aes(n,words)) +
  geom_col()
```

### **Question 4**

repeat question 2, but this time tokenize into bi-grams. how does the result change if you look at tri-grams?

```{r}
mt_samples %>%
  unnest_ngrams(ngram2, transcription, n = 2) %>%
  count(ngram2, sort=TRUE) %>%
  top_n(20,n)

mt_samples %>%
  unnest_ngrams(ngram3, transcription, n = 3) %>%
  count(ngram3, sort=TRUE) %>%
  top_n(20,n)
```

### **Question 5**

Using the results you got from questions 4. Pick a word and count the words that appears after and before it.

```{r}
library(tidyr)
mt_samples %>%
  unnest_ngrams(ngramb, transcription, n = 2) %>%
  separate(ngramb, into=c("word1","word2"), sep = " ")  %>%
  select(word1, word2)
```

### **Question 6**

Which words are most used in each of the specialties. you can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the most 5 used words?

```{r}
# grouping does the same medical specialty many times
mt_samples %>%
  unnest_tokens(words, transcription) %>%
  group_by(medical_specialty) %>%
  count(words) %>%
  top_n(20,n)
```

### **Question 7 - extra**

Find your own insight in the data:

Ideas:

-   Interesting ngrams

-   See if certain words are used more in some specialties then others

```{r}
mt_samples %>%
  unnest_tokens(texts, transcription) %>%
  count(texts, medical_specialty) %>%
  bind_tf_idf(texts,medical_specialty, n) %>%
  arrange(desc(tf_idf))
```
